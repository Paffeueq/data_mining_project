{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "dbdd748a",
      "metadata": {},
      "source": [
        "# Model regresji: przewidywanie `Life expectancy`\n",
        "\n",
        "Cel: zbudować i ocenić model regresji przewidujący oczekiwaną długość życia (*Life expectancy*) na podstawie zmiennych zdrowotnych, ekonomicznych i społecznych z pliku CSV.\n",
        "\n",
        "Notebook prowadzi krok po kroku:\n",
        "1) wczytanie danych, 2) kontrola braków i typów, 3) przygotowanie danych (uzupełnianie braków, standaryzacja, kodowanie `Status`), 4) dobór cech, 5) modele liniowe + regularyzacja (Ridge), 6) walidacja i analiza błędów, 7) interpretacja (permutacja), 8) zapis modelu.\n",
        "\n",
        "Założenie (domyślne): **nie używamy `Country` jako cechy**, żeby uniknąć “identyfikowania kraju” i przeuczenia; zostawiamy `Year`, `Status` i pozostałe zmienne numeryczne."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5664fa9f",
      "metadata": {},
      "outputs": [],
      "source": [
        "# 1) Ustawienia środowiska i import bibliotek\n",
        "from pathlib import Path\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "from sklearn.model_selection import train_test_split, GridSearchCV, cross_validate\n",
        "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
        "from sklearn.preprocessing import OneHotEncoder, StandardScaler\n",
        "from sklearn.impute import SimpleImputer\n",
        "\n",
        "from sklearn.linear_model import LinearRegression, Ridge\n",
        "\n",
        "# Ustawienia czytelności\n",
        "pd.set_option('display.max_columns', 200)\n",
        "pd.set_option('display.width', 140)\n",
        "\n",
        "RANDOM_STATE = 42\n",
        "np.random.seed(RANDOM_STATE)\n",
        "\n",
        "sns.set_theme(style=\"whitegrid\")\n",
        "\n",
        "print(\"OK: biblioteki załadowane\")\n",
        "print(\"Wskazówka: jeśli brakuje scikit-learn, doinstaluj: pip install scikit-learn\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7de7a959",
      "metadata": {},
      "source": [
        "## 2) Wczytanie CSV i szybka walidacja danych\n",
        "\n",
        "Uwaga: w oryginalnym zbiorze zdarzają się spacje na końcu nazw kolumn (np. `Life expectancy `). Poniżej **czyścimy nazwy kolumn** przez `.str.strip()` i dopiero potem wybieramy target."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "db041610",
      "metadata": {},
      "outputs": [],
      "source": [
        "# 2) Wczytanie CSV i szybka walidacja danych\n",
        "\n",
        "CSV_PATH = Path(\"Life Expectancy Data.csv\")  # plik jest w katalogu projektu\n",
        "\n",
        "if not CSV_PATH.exists():\n",
        "    raise FileNotFoundError(f\"Nie znaleziono pliku: {CSV_PATH.resolve()}\")\n",
        "\n",
        "df = pd.read_csv(CSV_PATH)\n",
        "\n",
        "# standaryzacja nazw kolumn (w zbiorze zdarzają się spacje na końcu)\n",
        "df.columns = df.columns.str.strip()\n",
        "\n",
        "print(\"Wczytano df:\")\n",
        "print(\"- shape:\", df.shape)\n",
        "display(df.head())\n",
        "\n",
        "# szybka sanity-check\n",
        "print(\"\\nKolumny:\", list(df.columns))\n",
        "print(\"\\nPrzykładowe kraje:\", df[\"Country\"].dropna().unique()[:5])\n",
        "print(\"Zakres lat:\", df[\"Year\"].min(), \"-\", df[\"Year\"].max())"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c68b530c",
      "metadata": {},
      "source": [
        "## 3) Podgląd struktury: typy kolumn, braki danych, duplikaty\n",
        "\n",
        "W tym kroku:\n",
        "- sprawdzamy typy kolumn,\n",
        "- liczymy braki danych (procentowo),\n",
        "- sprawdzamy duplikaty,\n",
        "- robimy prosty wykres procentu braków."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "33d8aa9d",
      "metadata": {},
      "outputs": [],
      "source": [
        "# 3) Typy, braki, duplikaty\n",
        "\n",
        "display(df.info())\n",
        "\n",
        "missing_frac = df.isna().mean().sort_values(ascending=False)\n",
        "missing_tbl = (missing_frac * 100).round(2).to_frame(\"missing_%\")\n",
        "missing_tbl[\"missing_count\"] = df.isna().sum()\n",
        "missing_tbl = missing_tbl[missing_tbl[\"missing_count\"] > 0]\n",
        "\n",
        "print(\"\\nKolumny z brakami (top 15):\")\n",
        "display(missing_tbl.head(15))\n",
        "\n",
        "dup_count = df.duplicated().sum()\n",
        "print(f\"\\nDuplikaty wierszy: {dup_count}\")\n",
        "\n",
        "cat_cols = df.select_dtypes(include=[\"object\"]).columns.tolist()\n",
        "print(\"\\nKolumny kategoryczne:\", cat_cols)\n",
        "for c in cat_cols:\n",
        "    print(f\"- {c}: {df[c].nunique(dropna=True)} unikalnych wartości\")\n",
        "\n",
        "# wykres procentu braków\n",
        "if len(missing_tbl) > 0:\n",
        "    plt.figure(figsize=(10, 5))\n",
        "    missing_tbl[\"missing_%\"].sort_values(ascending=True).plot(kind=\"barh\")\n",
        "    plt.title(\"Procent braków danych w kolumnach\")\n",
        "    plt.xlabel(\"% braków\")\n",
        "    plt.tight_layout()\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0a81bbe0",
      "metadata": {},
      "source": [
        "## Braki danych: co z nimi robimy?\n",
        "\n",
        "W EDA wyszło, że usunięcie wierszy z brakami wycina ~44% danych i może biasować próbę (kraje z lepszą sprawozdawczością). Dlatego tutaj:\n",
        "\n",
        "- **Nie usuwamy wierszy** z brakami w cechach (poza brakami w samym `y`).\n",
        "- Uzupełniamy braki w danych w prosty sposób:\n",
        "  - kolumny liczbowe → **mediana**,\n",
        "  - `Status` → **najczęstsza wartość** + zamiana na kolumny 0/1 (One-Hot).\n",
        "\n",
        "Opcjonalnie (zgodnie z narracją EDA o \"wartościach z lat poprzednich\"): można zrobić uzupełnianie **w obrębie kraju po czasie** (forward-fill/back-fill). To ma sens, ale trzeba uważać na wyciek informacji; jeśli chcesz, zrobimy to jako osobny eksperyment (train i test osobno)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0fc926aa",
      "metadata": {},
      "outputs": [],
      "source": [
        "def country_time_impute(frame: pd.DataFrame, numeric_columns: list[str]) -> pd.DataFrame:\n",
        "    out = frame.copy()\n",
        "    if \"Country\" not in out.columns or \"Year\" not in out.columns:\n",
        "        return out\n",
        "\n",
        "    out = out.sort_values([\"Country\", \"Year\"]).reset_index(drop=True)\n",
        "\n",
        "    # forward-fill i backward-fill w obrębie kraju\n",
        "    out[numeric_columns] = (\n",
        "        out.groupby(\"Country\", as_index=False)[numeric_columns]\n",
        "           .apply(lambda g: g.ffill().bfill())\n",
        "           .reset_index(drop=True)\n",
        "    )\n",
        "\n",
        "    return out\n",
        "\n",
        "print(\"OK: helper country_time_impute() gotowy (opcjonalny)\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "105ac91b",
      "metadata": {},
      "source": [
        "## 4) Wybór zmiennej celu (target) i podział na X/y\n",
        "\n",
        "Zmienna celu: **`Life expectancy`**.\n",
        "\n",
        "Zgodnie z EDA: **nie używamy `Country` jako cechy** (żeby nie robić modelu „rozpoznaj kraj”). `Country` zostaje tylko jako ewentualny klucz do analiz/eksperymentów z imputacją po czasie."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a581f915",
      "metadata": {},
      "outputs": [],
      "source": [
        "# 4) Target oraz X/y\n",
        "\n",
        "TARGET_COL = \"Life expectancy\"\n",
        "\n",
        "if TARGET_COL not in df.columns:\n",
        "    raise KeyError(f\"Nie ma kolumny targetu '{TARGET_COL}'. Dostępne: {list(df.columns)}\")\n",
        "\n",
        "# y musi być liczbowe\n",
        "y = pd.to_numeric(df[TARGET_COL], errors=\"coerce\")\n",
        "\n",
        "# X: wszystkie kolumny poza targetem, ale bez Country\n",
        "DROP_COLS = [\"Country\"]\n",
        "X = df.drop(columns=[TARGET_COL] + [c for c in DROP_COLS if c in df.columns])\n",
        "\n",
        "print(\"X shape:\", X.shape)\n",
        "print(\"y shape:\", y.shape)\n",
        "\n",
        "# szybki podgląd\n",
        "print(\"\\nX dtypes (top 10):\")\n",
        "print(X.dtypes.head(10))\n",
        "print(\"\\ny missing %:\", round(y.isna().mean() * 100, 2))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8843f0c7",
      "metadata": {},
      "source": [
        "## 5) Podział train/test (z kontrolą losowości)\n",
        "\n",
        "Żeby porównywać modele uczciwie, robimy stały podział `train_test_split`.\n",
        "\n",
        "Opcjonalnie: przy mocno skośnym rozkładzie targetu można robić pseudo-stratyfikację po binach `y` (tu robimy to „best-effort”: jeśli się nie da, wracamy do zwykłego split)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a3e4118b",
      "metadata": {},
      "outputs": [],
      "source": [
        "# 5) train/test split\n",
        "\n",
        "# Best-effort stratyfikacja po binach y (jeśli się da)\n",
        "stratify_labels = None\n",
        "try:\n",
        "    y_bins = pd.qcut(y, q=10, duplicates=\"drop\")\n",
        "    if y_bins.nunique() >= 2 and y_bins.notna().all():\n",
        "        stratify_labels = y_bins\n",
        "except Exception:\n",
        "    stratify_labels = None\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X,\n",
        "    y,\n",
        "    test_size=0.2,\n",
        "    random_state=RANDOM_STATE,\n",
        "    stratify=stratify_labels,\n",
        ")\n",
        "\n",
        "print(\"Train:\", X_train.shape, \"Test:\", X_test.shape)\n",
        "print(\"y_train missing %:\", round(y_train.isna().mean() * 100, 2))\n",
        "print(\"y_test missing %:\", round(y_test.isna().mean() * 100, 2))\n",
        "\n",
        "# Jeśli target ma braki, usuwamy je TYLKO z y i odpowiadających wierszy X w danym zbiorze.\n",
        "train_mask = y_train.notna()\n",
        "test_mask = y_test.notna()\n",
        "X_train = X_train.loc[train_mask]\n",
        "y_train = y_train.loc[train_mask]\n",
        "X_test = X_test.loc[test_mask]\n",
        "y_test = y_test.loc[test_mask]\n",
        "\n",
        "print(\"Po usunięciu braków w y:\")\n",
        "print(\"Train:\", X_train.shape, \"Test:\", X_test.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8ceb92e1",
      "metadata": {},
      "source": [
        "## 6) Przygotowanie danych: uzupełnianie braków + standaryzacja + kodowanie `Status`\n",
        "\n",
        "W tym kroku przygotowujemy dane do uczenia modelu:\n",
        "- kolumny liczbowe: uzupełnienie braków (mediana) i standaryzacja,\n",
        "- `Status`: uzupełnienie braków (najczęstsza wartość) i kodowanie One-Hot.\n",
        "\n",
        "Ważne: transformacje „uczymy” tylko na zbiorze treningowym i stosujemy je do zbioru testowego, żeby nie mieszać informacji z testu do treningu."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "088ccca8",
      "metadata": {},
      "outputs": [],
      "source": [
        "# 6) Przygotowanie danych (ręczne fit/transform)\n",
        "\n",
        "# Identyfikacja kolumn\n",
        "numeric_features = X_train.select_dtypes(include=[\"number\"]).columns.tolist()\n",
        "categorical_features = X_train.select_dtypes(include=[\"object\"]).columns.tolist()\n",
        "\n",
        "# W tym zbiorze sensownie zostawić tylko Status jako cechę kategoryczną\n",
        "categorical_features = [c for c in categorical_features if c == \"Status\"]\n",
        "\n",
        "print(\"Numeryczne:\", numeric_features)\n",
        "print(\"Kategoryczne:\", categorical_features)\n",
        "\n",
        "# Obiekty przygotowania danych\n",
        "num_imputer = SimpleImputer(strategy=\"median\")\n",
        "scaler = StandardScaler()\n",
        "\n",
        "cat_imputer = SimpleImputer(strategy=\"most_frequent\")\n",
        "\n",
        "# OneHotEncoder: kompatybilność między wersjami sklearn (sparse vs sparse_output)\n",
        "try:\n",
        "    ohe = OneHotEncoder(handle_unknown=\"ignore\", sparse_output=False)\n",
        "except TypeError:\n",
        "    ohe = OneHotEncoder(handle_unknown=\"ignore\", sparse=False)\n",
        "\n",
        "def fit_preprocess(X_train: pd.DataFrame) -> dict:\n",
        "    \"\"\"Dopasuj obiekty przygotowania danych na zbiorze treningowym.\"\"\"\n",
        "    bundle = {}\n",
        "\n",
        "    # numeryczne\n",
        "    bundle[\"numeric_features\"] = numeric_features\n",
        "    bundle[\"num_imputer\"] = num_imputer.fit(X_train[numeric_features])\n",
        "    X_num = bundle[\"num_imputer\"].transform(X_train[numeric_features])\n",
        "    bundle[\"scaler\"] = scaler.fit(X_num)\n",
        "\n",
        "    # kategoryczne (Status)\n",
        "    bundle[\"categorical_features\"] = categorical_features\n",
        "    if len(categorical_features) > 0:\n",
        "        bundle[\"cat_imputer\"] = cat_imputer.fit(X_train[categorical_features])\n",
        "        X_cat = bundle[\"cat_imputer\"].transform(X_train[categorical_features])\n",
        "        bundle[\"ohe\"] = ohe.fit(X_cat)\n",
        "    else:\n",
        "        bundle[\"cat_imputer\"] = None\n",
        "        bundle[\"ohe\"] = None\n",
        "\n",
        "    # nazwy cech po transformacji\n",
        "    num_names = np.array(numeric_features, dtype=object)\n",
        "    if bundle[\"ohe\"] is not None:\n",
        "        try:\n",
        "            cat_names = bundle[\"ohe\"].get_feature_names_out(categorical_features).astype(object)\n",
        "        except Exception:\n",
        "            cat_names = np.array([], dtype=object)\n",
        "    else:\n",
        "        cat_names = np.array([], dtype=object)\n",
        "    bundle[\"feature_names\"] = np.concatenate([num_names, cat_names])\n",
        "\n",
        "    return bundle\n",
        "\n",
        "def transform_preprocess(bundle: dict, X: pd.DataFrame) -> np.ndarray:\n",
        "    \"\"\"Zastosuj przygotowanie danych: imputacja + skalowanie + OneHot.\"\"\"\n",
        "    # numeryczne\n",
        "    X_num = bundle[\"num_imputer\"].transform(X[bundle[\"numeric_features\"]])\n",
        "    X_num = bundle[\"scaler\"].transform(X_num)\n",
        "\n",
        "    # kategoryczne\n",
        "    if bundle.get(\"ohe\") is not None and len(bundle.get(\"categorical_features\", [])) > 0:\n",
        "        X_cat = bundle[\"cat_imputer\"].transform(X[bundle[\"categorical_features\"]])\n",
        "        X_cat = bundle[\"ohe\"].transform(X_cat)\n",
        "    else:\n",
        "        X_cat = np.empty((len(X), 0))\n",
        "\n",
        "    X_t = np.hstack([X_num, X_cat])\n",
        "\n",
        "    # (opcjonalnie) dobór cech\n",
        "    if \"selector\" in bundle:\n",
        "        X_t = bundle[\"selector\"].transform(X_t)\n",
        "\n",
        "    return X_t\n",
        "\n",
        "preprocess_bundle = fit_preprocess(X_train)\n",
        "X_train_t = transform_preprocess(preprocess_bundle, X_train)\n",
        "X_test_t = transform_preprocess(preprocess_bundle, X_test)\n",
        "\n",
        "print(\"OK: przygotowanie danych gotowe\")\n",
        "print(\"- X_train_t:\", X_train_t.shape)\n",
        "print(\"- X_test_t:\", X_test_t.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "40dbe723",
      "metadata": {},
      "source": [
        "## 7) Dobór cech: SelectKBest (f_regression)\n",
        "\n",
        "Wymaganie projektu obejmuje dobór cech. Ponieważ po przygotowaniu danych mamy już macierz cech (`X_train_t`), wykonujemy dobór cech wyłącznie na zbiorze treningowym i stosujemy go na teście.\n",
        "\n",
        "Metoda: `SelectKBest` z testem `f_regression` (univariate) – wybiera cechy o najsilniejszym związku liniowym z targetem."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ef8e2470",
      "metadata": {},
      "outputs": [],
      "source": [
        "# 7) Dobór cech – SelectKBest\n",
        "from sklearn.feature_selection import SelectKBest, f_regression\n",
        "\n",
        "# Ustal liczbę wybieranych cech (prosty, rozsądny wybór do raportu)\n",
        "k = min(20, X_train_t.shape[1])\n",
        "selector = SelectKBest(score_func=f_regression, k=k)\n",
        "selector.fit(X_train_t, y_train)\n",
        "\n",
        "# Zastosuj dobór cech\n",
        "X_train_t_fs = selector.transform(X_train_t)\n",
        "X_test_t_fs = selector.transform(X_test_t)\n",
        "\n",
        "selected_mask = selector.get_support()\n",
        "selected_feature_names = preprocess_bundle[\"feature_names\"][selected_mask]\n",
        "\n",
        "# Podłączamy selector do bundle (żeby zapisany model dało się używać)\n",
        "preprocess_bundle[\"selector\"] = selector\n",
        "preprocess_bundle[\"feature_names\"] = selected_feature_names\n",
        "\n",
        "# Dla dalszych kroków pracujemy już na macierzy po doborze cech\n",
        "X_train_t = X_train_t_fs\n",
        "X_test_t = X_test_t_fs\n",
        "\n",
        "print(\"OK: dobór cech\")\n",
        "print(\"- k:\", k)\n",
        "print(\"- X_train_t:\", X_train_t.shape)\n",
        "print(\"- X_test_t:\", X_test_t.shape)\n",
        "display(pd.DataFrame({\"feature\": selected_feature_names}).head(20))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "772631d6",
      "metadata": {},
      "source": [
        "## 8) Funkcja ewaluacji (MAE / RMSE / $R^2$)\n",
        "\n",
        "Dalej trenujemy modele na danych po preprocessingu (`X_train_t`, `X_test_t`)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b03f0565",
      "metadata": {},
      "outputs": [],
      "source": [
        "# 8) Ewaluacja – bez Pipeline\n",
        "\n",
        "def evaluate_estimator(model, X_train_t, X_test_t, y_train, y_test, name: str) -> dict:\n",
        "    model.fit(X_train_t, y_train)\n",
        "    y_pred = model.predict(X_test_t)\n",
        "\n",
        "    mae = mean_absolute_error(y_test, y_pred)\n",
        "    rmse = np.sqrt(mean_squared_error(y_test, y_pred))\n",
        "    r2 = r2_score(y_test, y_pred)\n",
        "\n",
        "    return {\n",
        "        \"model\": name,\n",
        "        \"MAE\": mae,\n",
        "        \"RMSE\": rmse,\n",
        "        \"R2\": r2,\n",
        "    }\n",
        "\n",
        "results = []\n",
        "print(\"OK: evaluate_estimator() gotowa\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7d524eb1",
      "metadata": {},
      "source": [
        "## 8) Modele: dlaczego takie?\n",
        "\n",
        "Wybór modeli opiera się na wnioskach z EDA:\n",
        "\n",
        "- **Zależności są w dużej mierze liniowe** (korelacje + wykresy punktowe), więc startujemy od **regresji liniowej**.\n",
        "- Jest sporo **współzależnych cech** (np. szczepienia korelują), co może destabilizować współczynniki w czystej regresji liniowej.\n",
        "- Dlatego jako model „docelowy” bierzemy **Ridge Regression (L2)** – nadal liniowy, ale bardziej stabilny.\n",
        "\n",
        "Nie używamy `Pipeline`: preprocessing robimy osobno (fit na train → transform train/test), a modele trenujemy na macierzy cech po transformacji."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b4b0e8d7",
      "metadata": {},
      "outputs": [],
      "source": [
        "# 8) LinearRegression + Ridge (z tuningiem) – bez sklearn.Pipeline\n",
        "\n",
        "# LinearRegression\n",
        "lin_model = LinearRegression()\n",
        "results.append(evaluate_estimator(lin_model, X_train_t, X_test_t, y_train, y_test, \"LinearRegression\"))\n",
        "\n",
        "# Ridge z GridSearchCV (na danych po preprocessingu)\n",
        "ridge = Ridge(random_state=RANDOM_STATE)\n",
        "param_grid = {\"alpha\": np.logspace(-3, 3, 13)}\n",
        "\n",
        "ridge_search = GridSearchCV(\n",
        "    ridge,\n",
        "    param_grid=param_grid,\n",
        "    scoring=\"neg_root_mean_squared_error\",\n",
        "    cv=5,\n",
        "    n_jobs=-1,\n",
        " )\n",
        "\n",
        "ridge_search.fit(X_train_t, y_train)\n",
        "print(\"Ridge best params:\", ridge_search.best_params_)\n",
        "\n",
        "ridge_best = ridge_search.best_estimator_\n",
        "results.append(evaluate_estimator(ridge_best, X_train_t, X_test_t, y_train, y_test, \"Ridge(best alpha)\"))\n",
        "\n",
        "results_df = pd.DataFrame(results).sort_values(\"RMSE\")\n",
        "display(results_df)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4a6c9162",
      "metadata": {},
      "source": [
        "## 10) Walidacja krzyżowa i raport metryk\n",
        "\n",
        "Walidacja krzyżowa (np. 5-fold) pozwala sprawdzić stabilność wyniku.\n",
        "\n",
        "$$RMSE = \\sqrt{\\frac{1}{n}\\sum_{i=1}^{n}(y_i-\\hat{y}_i)^2}$$"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "bcc4a367",
      "metadata": {},
      "outputs": [],
      "source": [
        "# 11) Cross-validation dla najlepszego modelu (wg RMSE na teście) – bez pipeline\n",
        "\n",
        "best_name = results_df.iloc[0][\"model\"]\n",
        "print(\"Najlepszy wg RMSE (test):\", best_name)\n",
        "\n",
        "if best_name.startswith(\"Ridge\"):\n",
        "    best_model = ridge_best\n",
        "else:\n",
        "    best_model = lin_model\n",
        "\n",
        "scoring = {\n",
        "    \"MAE\": \"neg_mean_absolute_error\",\n",
        "    \"RMSE\": \"neg_root_mean_squared_error\",\n",
        "    \"R2\": \"r2\",\n",
        "}\n",
        "\n",
        "cv = cross_validate(\n",
        "    best_model,\n",
        "    X_train_t,\n",
        "    y_train,\n",
        "    cv=5,\n",
        "    scoring=scoring,\n",
        "    n_jobs=-1,\n",
        "    return_train_score=False,\n",
        " )\n",
        "\n",
        "cv_summary = pd.DataFrame(\n",
        "    {\n",
        "        \"MAE\": -cv[\"test_MAE\"],\n",
        "        \"RMSE\": -cv[\"test_RMSE\"],\n",
        "        \"R2\": cv[\"test_R2\"],\n",
        "    },\n",
        ")\n",
        "\n",
        "print(\"\\nCV (5-fold) – średnia ± odch. std:\")\n",
        "for metric in [\"MAE\", \"RMSE\", \"R2\"]:\n",
        "    print(f\"- {metric}: {cv_summary[metric].mean():.4f} ± {cv_summary[metric].std():.4f}\")\n",
        "\n",
        "display(cv_summary)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "56300230",
      "metadata": {},
      "source": [
        "## 11) Analiza błędów: $y_{true}$ vs $y_{pred}$ i reszty\n",
        "\n",
        "Wykresy pozwalają szybko sprawdzić:\n",
        "- czy błąd rośnie dla pewnych zakresów (heteroscedastyczność),\n",
        "- czy są obserwacje odstające,\n",
        "- czy model ma bias (systematyczne przeszacowanie/niedoszacowanie)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "705442e2",
      "metadata": {},
      "outputs": [],
      "source": [
        "# 11) Wykresy błędów na teście (bez pipeline)\n",
        "\n",
        "best_model.fit(X_train_t, y_train)\n",
        "y_pred = best_model.predict(X_test_t)\n",
        "residuals = y_test - y_pred\n",
        "\n",
        "plt.figure(figsize=(6, 6))\n",
        "plt.scatter(y_test, y_pred, alpha=0.6)\n",
        "min_v = min(y_test.min(), y_pred.min())\n",
        "max_v = max(y_test.max(), y_pred.max())\n",
        "plt.plot([min_v, max_v], [min_v, max_v], \"r--\", linewidth=2)\n",
        "plt.xlabel(\"y_true\")\n",
        "plt.ylabel(\"y_pred\")\n",
        "plt.title(\"Predykcja vs rzeczywiste (test)\")\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "plt.figure(figsize=(8, 4))\n",
        "sns.histplot(residuals, kde=True, bins=30)\n",
        "plt.title(\"Rozkład reszt (y_true - y_pred)\")\n",
        "plt.xlabel(\"reszta\")\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "plt.figure(figsize=(8, 4))\n",
        "plt.scatter(y_pred, residuals, alpha=0.6)\n",
        "plt.axhline(0, color=\"red\", linestyle=\"--\")\n",
        "plt.title(\"Reszty vs y_pred\")\n",
        "plt.xlabel(\"y_pred\")\n",
        "plt.ylabel(\"reszta\")\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "print(\"Reszty: średnia=\", float(np.mean(residuals)), \"std=\", float(np.std(residuals)))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "990d823a",
      "metadata": {},
      "source": [
        "## 12) Ważność cech (Permutation Importance)\n",
        "\n",
        "Ważność cech liczymy przez permutację na zbiorze testowym:\n",
        "- mieszamy jedną kolumnę w `X_test_t`,\n",
        "- patrzymy, jak bardzo pogarsza się wynik (tu: RMSE).\n",
        "\n",
        "To działa dla modeli liniowych i nieliniowych."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0596ef9e",
      "metadata": {},
      "outputs": [],
      "source": [
        "# 12) Permutation importance (bez pipeline)\n",
        "from sklearn.inspection import permutation_importance\n",
        "\n",
        "# dopasuj najlepszy model na train\n",
        "best_model.fit(X_train_t, y_train)\n",
        "\n",
        "# nazwy cech po preprocessingu (z naszego \"ręcznego\" preprocessu)\n",
        "feature_names = np.array(preprocess_bundle.get(\"feature_names\", []), dtype=object)\n",
        "\n",
        "perm = permutation_importance(\n",
        "    best_model,\n",
        "    X_test_t,\n",
        "    y_test,\n",
        "    n_repeats=10,\n",
        "    random_state=RANDOM_STATE,\n",
        "    scoring=\"neg_root_mean_squared_error\",\n",
        ")\n",
        "\n",
        "importances = pd.DataFrame(\n",
        "    {\n",
        "        \"feature\": feature_names,\n",
        "        \"importance_mean\": perm.importances_mean,\n",
        "        \"importance_std\": perm.importances_std,\n",
        "    }\n",
        ").sort_values(\"importance_mean\", ascending=False)\n",
        "\n",
        "display(importances.head(20))\n",
        "\n",
        "plt.figure(figsize=(10, 6))\n",
        "topk = importances.head(15).iloc[::-1]\n",
        "plt.barh(topk[\"feature\"], topk[\"importance_mean\"], xerr=topk[\"importance_std\"])\n",
        "plt.title(\"Permutation importance (top 15) – spadek jakości po permutacji cechy\")\n",
        "plt.xlabel(\"Zmiana (neg_RMSE) – im większa wartość bezwzględna, tym ważniejsza cecha\")\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ac302a8b",
      "metadata": {},
      "source": [
        "## 14) Sprawdzenie uogólniania: podział po krajach (Country-holdout)\n",
        "\n",
        "W EDA dane są w układzie **kraj–rok**. Przy losowym podziale wierszy ten sam kraj trafia do train i test, co często **zawyża wynik**.\n",
        "\n",
        "Dlatego robimy dodatkowy test: trenujemy na części krajów i testujemy na *innych* krajach. To mówi, jak model działa na „nowych” państwach."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ac77465c",
      "metadata": {},
      "outputs": [],
      "source": [
        "# 15) Country-holdout (bez Pipeline)\n",
        "\n",
        "# przygotuj pełne X/y + grupy\n",
        "_df = df.copy()\n",
        "_df.columns = _df.columns.str.strip()\n",
        "\n",
        "# target\n",
        "_y_all = pd.to_numeric(_df[TARGET_COL], errors=\"coerce\")\n",
        "\n",
        "# cechy bez Country\n",
        "_X_all = _df.drop(columns=[TARGET_COL, \"Country\"], errors=\"ignore\")\n",
        "\n",
        "# usuń braki w y\n",
        "mask_all = _y_all.notna()\n",
        "_X_all = _X_all.loc[mask_all].reset_index(drop=True)\n",
        "_y_all = _y_all.loc[mask_all].reset_index(drop=True)\n",
        "_groups = _df.loc[mask_all, \"Country\"].reset_index(drop=True)\n",
        "\n",
        "countries = _groups.unique()\n",
        "rng = np.random.RandomState(RANDOM_STATE)\n",
        "rng.shuffle(countries)\n",
        "\n",
        "cut = int(0.8 * len(countries))\n",
        "train_countries = set(countries[:cut])\n",
        "\n",
        "is_train = _groups.isin(train_countries)\n",
        "\n",
        "X_train_c = _X_all.loc[is_train].reset_index(drop=True)\n",
        "y_train_c = _y_all.loc[is_train].reset_index(drop=True)\n",
        "X_test_c = _X_all.loc[~is_train].reset_index(drop=True)\n",
        "y_test_c = _y_all.loc[~is_train].reset_index(drop=True)\n",
        "\n",
        "print(\"Country-holdout:\")\n",
        "print(\"- train rows:\", X_train_c.shape, \"test rows:\", X_test_c.shape)\n",
        "print(\"- train countries:\", len(train_countries), \"test countries:\", len(countries) - len(train_countries))\n",
        "\n",
        "# dopasuj preprocess na train_c\n",
        "numeric_features_c = X_train_c.select_dtypes(include=[\"number\"]).columns.tolist()\n",
        "categorical_features_c = [c for c in X_train_c.select_dtypes(include=[\"object\"]).columns.tolist() if c == \"Status\"]\n",
        "\n",
        "num_imputer_c = SimpleImputer(strategy=\"median\")\n",
        "scaler_c = StandardScaler()\n",
        "cat_imputer_c = SimpleImputer(strategy=\"most_frequent\")\n",
        "try:\n",
        "    ohe_c = OneHotEncoder(handle_unknown=\"ignore\", sparse_output=False)\n",
        "except TypeError:\n",
        "    ohe_c = OneHotEncoder(handle_unknown=\"ignore\", sparse=False)\n",
        "\n",
        "# fit\n",
        "Xtr_num = scaler_c.fit_transform(num_imputer_c.fit_transform(X_train_c[numeric_features_c]))\n",
        "Xte_num = scaler_c.transform(num_imputer_c.transform(X_test_c[numeric_features_c]))\n",
        "\n",
        "if len(categorical_features_c) > 0:\n",
        "    Xtr_cat = ohe_c.fit_transform(cat_imputer_c.fit_transform(X_train_c[categorical_features_c]))\n",
        "    Xte_cat = ohe_c.transform(cat_imputer_c.transform(X_test_c[categorical_features_c]))\n",
        "else:\n",
        "    Xtr_cat = np.empty((len(X_train_c), 0))\n",
        "    Xte_cat = np.empty((len(X_test_c), 0))\n",
        "\n",
        "X_train_ct = np.hstack([Xtr_num, Xtr_cat])\n",
        "X_test_ct = np.hstack([Xte_num, Xte_cat])\n",
        "\n",
        "# (opcjonalnie) ten sam dobór cech co w głównym eksperymencie\n",
        "k_ct = min(k, X_train_ct.shape[1]) if \"k\" in globals() else min(20, X_train_ct.shape[1])\n",
        "selector_ct = SelectKBest(score_func=f_regression, k=k_ct)\n",
        "selector_ct.fit(X_train_ct, y_train_c)\n",
        "X_train_ct = selector_ct.transform(X_train_ct)\n",
        "X_test_ct = selector_ct.transform(X_test_ct)\n",
        "\n",
        "# porównanie modeli na country-holdout (liniowe + regularizacja)\n",
        "country_results = []\n",
        "country_results.append(evaluate_estimator(LinearRegression(), X_train_ct, X_test_ct, y_train_c, y_test_c, \"LinearRegression\"))\n",
        "\n",
        "ridge_c = GridSearchCV(\n",
        "    Ridge(random_state=RANDOM_STATE),\n",
        "    param_grid={\"alpha\": np.logspace(-3, 3, 13)},\n",
        "    scoring=\"neg_root_mean_squared_error\",\n",
        "    cv=5,\n",
        "    n_jobs=-1,\n",
        ")\n",
        "ridge_c.fit(X_train_ct, y_train_c)\n",
        "country_results.append(evaluate_estimator(ridge_c.best_estimator_, X_train_ct, X_test_ct, y_train_c, y_test_c, f\"Ridge(alpha={ridge_c.best_params_['alpha']})\"))\n",
        "\n",
        "display(pd.DataFrame(country_results).sort_values(\"RMSE\"))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7bab891c",
      "metadata": {},
      "source": [
        "## 15) Zapis: model + preprocess (bez Pipeline)\n",
        "\n",
        "Zapisujemy bundle z:\n",
        "- dopasowanymi transformatorami preprocessingu (`preprocess_bundle`),\n",
        "- najlepszym modelem (`best_model`)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2d15da8e",
      "metadata": {},
      "outputs": [],
      "source": [
        "# 15) Zapis bundle (model + preprocess) i przykład predykcji\n",
        "import joblib\n",
        "\n",
        "MODEL_PATH = Path(\"regression_model_and_preprocess.joblib\")\n",
        "\n",
        "bundle = {\n",
        "    \"preprocess_bundle\": preprocess_bundle,\n",
        "    \"model\": best_model,\n",
        "}\n",
        "\n",
        "joblib.dump(bundle, MODEL_PATH)\n",
        "print(\"Zapisano bundle do:\", MODEL_PATH.resolve())\n",
        "\n",
        "loaded = joblib.load(MODEL_PATH)\n",
        "\n",
        "sample = X_test.head(5)\n",
        "sample_t = transform_preprocess(loaded[\"preprocess_bundle\"], sample)\n",
        "preds = loaded[\"model\"].predict(sample_t)\n",
        "\n",
        "preview = pd.DataFrame({\n",
        "    \"y_true\": y_test.loc[sample.index].values,\n",
        "    \"y_pred\": preds,\n",
        "})\n",
        "preview[\"abs_error\"] = np.abs(preview[\"y_true\"] - preview[\"y_pred\"])\n",
        "display(preview)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "68c2578c",
      "metadata": {},
      "source": [
        "## 16) Wnioski do sprawozdania (regresja + dobór cech + regularyzacja)\n",
        "\n",
        "- **Wyniki są spójne z EDA**: wykresy/korelacje sugerowały w dużej mierze liniowe zależności → modele liniowe osiągają wysokie $R^2$ i rozsądny błąd (RMSE).\n",
        "- **Dobór cech (SelectKBest)** po preprocessingu pozwala ograniczyć liczbę zmiennych do tych najsilniej związanych liniowo z targetem; to upraszcza model i raport, bez zmiany założeń (model nadal jest liniowy).\n",
        "- **Ridge (L2) jest preferowany jako model główny**: regularyzacja stabilizuje współczynniki przy współliniowości cech (co było widoczne w EDA) i zwykle poprawia uogólnianie.\n",
        "- **Preprocessing jest kluczowy**: imputacja braków + skalowanie cech numerycznych + One-Hot dla `Status` umożliwia trening bez utraty dużej części danych (które wypadłyby przy `dropna`).\n",
        "- **Test country-holdout** (inne kraje w teście) jest trudniejszy i często daje gorsze metryki niż losowy split, co jest typowe dla danych kraj–rok."
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.11"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
